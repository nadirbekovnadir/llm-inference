# LLM Inference Playground

–õ–æ–∫–∞–ª—å–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å LLM –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º vLLM –∏ llama.cpp. –ü—Ä–æ–µ–∫—Ç –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

## üéØ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- **vLLM** - –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —á–µ—Ä–µ–∑ PagedAttention
- **llama.cpp** - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π CPU/GPU –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å GGUF –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π
- **Benchmark** - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –±—ç–∫–µ–Ω–¥–æ–≤
- **–ì–∏–±–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è** - CPU offloading, –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è, memory management

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
llm-inference/
‚îú‚îÄ‚îÄ venv-vllm/          # Python –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è vLLM
‚îú‚îÄ‚îÄ llama.cpp/          # llama.cpp —Å CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
‚îú‚îÄ‚îÄ models/             # –°–∫–∞—á–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îú‚îÄ‚îÄ hf/            # HuggingFace –º–æ–¥–µ–ª–∏ (–¥–ª—è vLLM)
‚îÇ   ‚îî‚îÄ‚îÄ gguf/          # GGUF –º–æ–¥–µ–ª–∏ (–¥–ª—è llama.cpp)
‚îú‚îÄ‚îÄ benchmark/         # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞
‚îÇ   ‚îú‚îÄ‚îÄ benchmark.py   # –°–∫—Ä–∏–ø—Ç –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
‚îÇ   ‚îú‚îÄ‚îÄ compare_results.py  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
‚îÇ   ‚îî‚îÄ‚îÄ results/       # –°–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
‚îî‚îÄ‚îÄ README.md
```

## üîß –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- **GPU**: NVIDIA —Å 16+ GB VRAM (—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–æ—Å—å –Ω–∞ RTX 4080 SUPER)
- **RAM**: 32+ GB (62+ GB –¥–ª—è CPU offloading –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π)
- **CUDA**: 12.4+
- **OS**: Linux / WSL2
- **Python**: 3.12+
- **System Packages**: `build-essential`, `python3-dev`, `cmake` (–¥–ª—è —Å–±–æ—Ä–∫–∏ llama.cpp)

## üöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º—ã

–ü–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ä–∞–±–æ—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø–∞–∫–µ—Ç—ã:

```bash
sudo apt update
sudo apt install build-essential python3-dev cmake git
```

### 2. –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è

```bash
git clone https://github.com/your-username/llm-inference.git
cd llm-inference
```

### 3. vLLM

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É—è uv –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
# –ï—Å–ª–∏ uv –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: pip install uv
uv venv venv-vllm
source venv-vllm/bin/activate

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
uv pip install vllm httpx

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
python -c "from vllm import LLM; print('vLLM OK')"
```

**–ò–∑–≤–µ—Å—Ç–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞ WSL2**: vLLM v0.13.0 –∏–º–µ–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å v1 engine –Ω–∞ WSL2. –í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è:
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `VLLM_ENABLE_V1_MULTIPROCESSING=0`
- –û—Ç–∫–∞—Ç–∏—Ç—å—Å—è –Ω–∞ vLLM v0.6.x
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—Ç–∏–≤–Ω—ã–π Linux

–ü–æ–¥—Ä–æ–±–Ω–µ–µ: [vLLM Troubleshooting](https://docs.vllm.ai/en/latest/usage/troubleshooting/)

### 4. llama.cpp

–î–ª—è —Ä–∞–±–æ—Ç—ã llama.cpp –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–∫–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ CUDA).

```bash
# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ llama.cpp –≤–Ω—É—Ç—Ä—å –ø—Ä–æ–µ–∫—Ç–∞
git clone https://github.com/ggml-org/llama.cpp

cd llama.cpp

# –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä–æ–π —Å–±–æ—Ä–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
rm -rf build

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å CUDA
cmake -B build -DGGML_CUDA=ON

# –°–±–æ—Ä–∫–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —è–¥—Ä–∞ CPU)
cmake --build build --config Release -j$(nproc)

# –ü—Ä–æ–≤–µ—Ä–∫–∞
./build/bin/llama-server --version
```

### 5. –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞

```bash
# –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –≤ –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
cd ..

source venv-vllm/bin/activate
pip install httpx
```

## üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

### vLLM (HuggingFace —Ñ–æ—Ä–º–∞—Ç)

```bash
source venv-vllm/bin/activate

# Qwen3-8B-AWQ (~5GB) - 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
huggingface-cli download Qwen/Qwen3-8B-AWQ --local-dir models/hf/Qwen3-8B-AWQ

# Qwen3-32B-FP8 (~35GB) - 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
huggingface-cli download Qwen/Qwen3-32B-FP8 --local-dir models/hf/Qwen3-32B-FP8

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏
huggingface-cli download meta-llama/Llama-3.1-8B-Instruct --local-dir models/hf/Llama-3.1-8B
huggingface-cli download Qwen/Qwen3-14B --local-dir models/hf/Qwen3-14B
```

### llama.cpp (GGUF —Ñ–æ—Ä–º–∞—Ç)

```bash
# Qwen3-8B Q4_K_M (~5GB) - 4-bit
huggingface-cli download Qwen/Qwen3-8B-GGUF Qwen3-8B-Q4_K_M.gguf --local-dir models/gguf

# Qwen3-8B Q8_0 (~9GB) - 8-bit (–ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
huggingface-cli download Qwen/Qwen3-8B-GGUF Qwen3-8B-Q8_0.gguf --local-dir models/gguf

# Qwen3-32B Q8_0 (~35GB) - 8-bit
huggingface-cli download Qwen/Qwen3-32B-GGUF Qwen3-32B-Q8_0.gguf --local-dir models/gguf

# Qwen3-32B Q4_K_M (~20GB) - 4-bit (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
huggingface-cli download Qwen/Qwen3-32B-GGUF Qwen3-32B-Q4_K_M.gguf --local-dir models/gguf
```

**–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –º–æ–¥–µ–ª–µ–π:**
- [Qwen3 8B GGUF](https://huggingface.co/Qwen/Qwen3-8B-GGUF)
- [Qwen3 32B GGUF](https://huggingface.co/Qwen/Qwen3-32B-GGUF)
- [Qwen3 Collection](https://huggingface.co/collections/Qwen/qwen3)

## üéÆ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### vLLM Server

```bash
source venv-vllm/bin/activate

# –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—É—Å–∫ (–º–æ–¥–µ–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤ GPU)
vllm serve models/hf/Qwen3-8B-AWQ \
    --port 8000 \
    --gpu-memory-utilization 0.90

# –° CPU offloading (–¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π)
vllm serve models/hf/Qwen3-32B-FP8 \
    --port 8000 \
    --cpu-offload-gb 20 \
    --gpu-memory-utilization 0.95

# –ò–∑ HuggingFace –Ω–∞–ø—Ä—è–º—É—é
vllm serve Qwen/Qwen3-8B-AWQ --port 8000
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `--gpu-memory-utilization` - –ø—Ä–æ—Ü–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è VRAM (0.7-0.95)
- `--cpu-offload-gb` - —Å–∫–æ–ª—å–∫–æ GB –≤—ã–≥—Ä—É–∂–∞—Ç—å –≤ RAM
- `--max-model-len` - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- `--tensor-parallel-size` - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU –¥–ª—è tensor parallelism

**–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: [vLLM Serving](https://docs.vllm.ai/en/stable/serving/distributed_serving/)

### llama.cpp Server

```bash
# –í—Å–µ —Å–ª–æ–∏ –≤ GPU
./llama.cpp/build/bin/llama-server \
    --model models/gguf/Qwen3-8B-Q4_K_M.gguf \
    --n-gpu-layers -1 \
    --ctx-size 4096 \
    --port 8001 \
    --host 0.0.0.0

# –ß–∞—Å—Ç–∏—á–Ω—ã–π GPU offload (30 —Å–ª–æ—ë–≤ –∏–∑ 64)
./llama.cpp/build/bin/llama-server \
    --model models/gguf/Qwen3-32B-Q8_0.gguf \
    --n-gpu-layers 30 \
    --ctx-size 4096 \
    --port 8001

# –° –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏
./llama.cpp/build/bin/llama-server \
    --model models/gguf/Qwen3-8B-Q8_0.gguf \
    --n-gpu-layers -1 \
    --parallel 4 \
    --port 8001
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `--n-gpu-layers` - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ –≤ GPU (-1 = –≤—Å–µ)
- `--ctx-size` - —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- `--parallel` - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
- `--threads` - CPU –ø–æ—Ç–æ–∫–∏ –¥–ª—è –Ω–µ-GPU –æ–ø–µ—Ä–∞—Ü–∏–π

**–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: [llama.cpp Server](https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md)

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API

```bash
# vLLM (–ø–æ—Ä—Ç 8000)
curl http://localhost:8000/v1/models

curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-8B-AWQ",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 100
  }'

# llama.cpp (–ø–æ—Ä—Ç 8001)
curl http://localhost:8001/v1/models

curl http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 100
  }'
```

## üìä –ë–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥

–ü–æ–¥—Ä–æ–±–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –≤ [benchmark/README.md](benchmark/README.md)

```bash
source venv-vllm/bin/activate
# (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) cd –≤ –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –µ—Å–ª–∏ –≤—ã –Ω–µ —Ç–∞–º

# –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–∞–ª–∞—Ö
# –¢–µ—Ä–º–∏–Ω–∞–ª 1: vLLM –Ω–∞ –ø–æ—Ä—Ç—É 8000
# –¢–µ—Ä–º–∏–Ω–∞–ª 2: llama.cpp –Ω–∞ –ø–æ—Ä—Ç—É 8001

# –¢–µ—Å—Ç –æ–¥–Ω–æ–≥–æ –±—ç–∫–µ–Ω–¥–∞
python benchmark/benchmark.py \
    --backend vllm \
    --scenario "qwen3-8b-gpu" \
    --prompt short

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±–æ–∏—Ö (–µ—Å–ª–∏ –∑–∞–ø—É—â–µ–Ω—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –∏–∑-–∑–∞ –ø–∞–º—è—Ç–∏)
# –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
python benchmark/compare_results.py \
    benchmark/results/benchmark_vllm_*.json \
    benchmark/results/benchmark_llamacpp_*.json
```

**–ú–µ—Ç—Ä–∏–∫–∏:**
- **TTFT** (Time to First Token) - –≤—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
- **TPS** (Tokens Per Second) - —Å–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- **ITL** (Inter-Token Latency) - –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏
- **E2E Latency** - –ø–æ–ª–Ω–æ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

## üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤

| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | vLLM | llama.cpp |
|----------------|------|-----------|
| **–§–æ—Ä–º–∞—Ç** | HuggingFace (safetensors) | GGUF |
| **–£—Å—Ç–∞–Ω–æ–≤–∫–∞** | Python package | –ö–æ–º–ø–∏–ª—è—Ü–∏—è –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤ |
| **CPU Offload** | `--cpu-offload-gb` | `--n-gpu-layers` |
| **–ú–µ—Ö–∞–Ω–∏–∑–º** | –°—Ç—Ä–∏–º–∏–Ω–≥ –≤–µ—Å–æ–≤ CPU‚ÜíGPU | –ü–æ—Å–ª–æ–π–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ |
| **Batch throughput** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê –û—Ç–ª–∏—á–Ω–æ | ‚≠ê‚≠ê‚≠ê –•–æ—Ä–æ—à–æ |
| **Single request** | ‚≠ê‚≠ê‚≠ê‚≠ê –•–æ—Ä–æ—à–æ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê –û—Ç–ª–∏—á–Ω–æ |
| **–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è** | AWQ, GPTQ, FP8 | Q2-Q8, FP16 |
| **Memory efficient** | ‚≠ê‚≠ê‚≠ê –°—Ä–µ–¥–Ω–µ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê –û—Ç–ª–∏—á–Ω–æ |
| **WSL2 —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** | ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã –≤ v0.13.0 | ‚úÖ –û—Ç–ª–∏—á–Ω–æ |

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å vLLM

- –í—ã—Å–æ–∫–∞—è –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å (batch inference)
- Production deployment —Å API
- –ú–æ–¥–µ–ª–∏ –≤ HuggingFace —Ñ–æ—Ä–º–∞—Ç–µ
- Tensor parallelism –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å llama.cpp

- CPU/GPU –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º
- –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ —á–µ—Ä–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é
- Single request latency –∫—Ä–∏—Ç–∏—á–Ω–∞
- WSL2 / Windows –æ–∫—Ä—É–∂–µ–Ω–∏–µ
- –ú–æ–¥–µ–ª–∏ –≤ GGUF —Ñ–æ—Ä–º–∞—Ç–µ

## üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

### –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- [vLLM Documentation](https://docs.vllm.ai/)
- [vLLM Installation Guide](https://docs.vllm.ai/en/stable/getting_started/installation/gpu/)
- [vLLM Serving](https://docs.vllm.ai/en/stable/serving/distributed_serving/)
- [llama.cpp GitHub](https://github.com/ggml-org/llama.cpp)
- [llama.cpp Server Guide](https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md)

### –ú–æ–¥–µ–ª–∏

- [Qwen3 Models](https://huggingface.co/collections/Qwen/qwen3)
- [Qwen3 Technical Report](https://qwenlm.github.io/blog/qwen3/)
- [HuggingFace Model Hub](https://huggingface.co/models)

### –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è

- [GGUF Quantization Guide](https://github.com/ggerganov/llama.cpp/blob/master/docs/quantization.md)
- [AWQ Quantization](https://github.com/mit-han-lab/llm-awq)
- [Qwen GGUF Documentation](https://qwen.readthedocs.io/en/latest/quantization/gguf.html)

### –ë–µ–Ω—á–º–∞—Ä–∫–∏

- [vLLM Performance](https://blog.vllm.ai/2023/06/20/vllm.html)
- [LLM Benchmarking Guide](https://developer.nvidia.com/blog/llm-benchmarking-fundamental-concepts/)
- [Anyscale Benchmarking](https://docs.anyscale.com/llm/serving/benchmarking/metrics)

### Troubleshooting

- [vLLM Troubleshooting](https://docs.vllm.ai/en/latest/usage/troubleshooting/)
- [vLLM GitHub Issues](https://github.com/vllm-project/vllm/issues)
- [llama.cpp Discussions](https://github.com/ggml-org/llama.cpp/discussions)
- [CUDA on WSL Guide](https://docs.nvidia.com/cuda/wsl-user-guide/)

## üí° –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### Python API (vLLM)

```python
from vllm import LLM, SamplingParams

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
llm = LLM(
    model="models/hf/Qwen3-8B-AWQ",
    gpu_memory_utilization=0.9
)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
prompts = ["Explain quantum computing in simple terms."]
sampling_params = SamplingParams(
    temperature=0.7,
    max_tokens=200
)

outputs = llm.generate(prompts, sampling_params)
for output in outputs:
    print(output.outputs[0].text)
```

### Python API (llama.cpp)

```python
from llama_cpp import Llama

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
llm = Llama(
    model_path="models/gguf/Qwen3-8B-Q4_K_M.gguf",
    n_gpu_layers=-1,
    n_ctx=4096
)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
response = llm(
    "Explain quantum computing in simple terms.",
    max_tokens=200,
    temperature=0.7
)

print(response["choices"][0]["text"])
```

### OpenAI Compatible Client

```python
from openai import OpenAI

# vLLM
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"
)

# llama.cpp
# client = OpenAI(
#     base_url="http://localhost:8001/v1",
#     api_key="dummy"
# )

response = client.chat.completions.create(
    model="Qwen/Qwen3-8B-AWQ",
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    max_tokens=100
)

print(response.choices[0].message.content)
```

## üõ†Ô∏è Troubleshooting

### vLLM –Ω–∞ WSL2 –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è

**–ü—Ä–æ–±–ª–µ–º–∞**: `RuntimeError: Engine core initialization failed`

**–†–µ—à–µ–Ω–∏–µ**:
```bash
export VLLM_ENABLE_V1_MULTIPROCESSING=0
vllm serve models/hf/Qwen3-8B-AWQ --port 8000
```

–ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –±–æ–ª–µ–µ —Å—Ç–∞—Ä—É—é –≤–µ—Ä—Å–∏—é:
```bash
pip install vllm==0.6.3
```

### Out of Memory (OOM) –æ—à–∏–±–∫–∏

**vLLM**:
```bash
# –£–º–µ–Ω—å—à–∏—Ç–µ gpu-memory-utilization
vllm serve model --gpu-memory-utilization 0.7

# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CPU offload
vllm serve model --cpu-offload-gb 10

# –£–º–µ–Ω—å—à–∏—Ç–µ context length
vllm serve model --max-model-len 2048
```

**llama.cpp**:
```bash
# –£–º–µ–Ω—å—à–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU —Å–ª–æ—ë–≤
llama-server --model model.gguf --n-gpu-layers 20

# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é (Q4 –≤–º–µ—Å—Ç–æ Q8)
```

### –ú–µ–¥–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è

1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –º–æ–¥–µ–ª—å –≤ GPU:
```bash
nvidia-smi  # –î–æ–ª–∂–Ω–∞ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
```

2. –î–ª—è llama.cpp —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ CUDA —Ä–∞–±–æ—Ç–∞–µ—Ç:
```bash
./llama-server --version  # –î–æ–ª–∂–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å CUDA
```

3. –£–º–µ–Ω—å—à–∏—Ç–µ batch size / parallel requests

## üìù TODO

- [ ] –î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É Ollama
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- [ ] WebUI –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏
- [ ] –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LangChain
- [ ] Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –¥–ª—è vLLM –∏ llama.cpp
- [ ] –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –±—ç–∫–µ–Ω–¥–∞–º–∏ (TensorRT-LLM, ExLlamaV2)

## üìÑ –õ–∏—Ü–µ–Ω–∑–∏—è

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Å–æ–∑–¥–∞–Ω –≤ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª—è—Ö. –ú–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç —Å–≤–æ–∏ –ª–∏—Ü–µ–Ω–∑–∏–∏:
- Qwen3: Apache 2.0
- Llama 3: Meta License

---

**–°–æ–∑–¥–∞–Ω–æ**: 2026-01-09
**–°–∏—Å—Ç–µ–º–∞**: Ubuntu 24.04 (WSL2), RTX 4080 SUPER, CUDA 12.4
