# LLM Inference Benchmark

–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ vLLM –∏ llama.cpp.

## üìä –ú–µ—Ç—Ä–∏–∫–∏

### TTFT (Time to First Token)
–í—Ä–µ–º—è –æ—Ç –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø—Ä–æ–º–ø—Ç–∞ –¥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.
- **–ò–∑–º–µ—Ä—è–µ—Ç—Å—è**: –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö (ms)
- **–í–∞–∂–Ω–æ—Å—Ç—å**: –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π (—á–∞—Ç–±–æ—Ç—ã, coding assistants)
- **–§–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è–Ω–∏—è**: –¥–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞, —Ä–∞–∑–º–µ—Ä batch, prefill optimization

### TPS (Tokens Per Second)
–°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.
- **–ò–∑–º–µ—Ä—è–µ—Ç—Å—è**: tokens/second
- **–í–∞–∂–Ω–æ—Å—Ç—å**: –û—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
- **–§–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è–Ω–∏—è**: —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏, –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è, GPU/CPU balance

### ITL (Inter-Token Latency)
–°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏.
- **–ò–∑–º–µ—Ä—è–µ—Ç—Å—è**: –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö (ms)
- **–§–æ—Ä–º—É–ª–∞**: (E2E Time - TTFT) / (Output Tokens - 1)
- **–í–∞–∂–Ω–æ—Å—Ç—å**: –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

### E2E Latency (End-to-End)
–ü–æ–ª–Ω–æ–µ –≤—Ä–µ–º—è –æ—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
- **–ò–∑–º–µ—Ä—è–µ—Ç—Å—è**: –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö (ms)
- **–§–æ—Ä–º—É–ª–∞**: TTFT + (Output Tokens √ó ITL)
- **–í–∞–∂–Ω–æ—Å—Ç—å**: –í–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ–º–∞—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º latency

## üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### 1. –†–∞–∑–¥–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

–ò–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π GPU –ø–∞–º—è—Ç–∏, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –±—ç–∫–µ–Ω–¥—ã –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏:

#### –®–∞–≥ 1: –¢–µ—Å—Ç vLLM

```bash
# –¢–µ—Ä–º–∏–Ω–∞–ª 1: –ó–∞–ø—É—Å–∫ vLLM —Å–µ—Ä–≤–µ—Ä–∞
source venv-vllm/bin/activate
vllm serve models/hf/Qwen3-8B-AWQ --port 8000

# –¢–µ—Ä–º–∏–Ω–∞–ª 2: –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞
source venv-vllm/bin/activate
# (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) cd –≤ –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞

python benchmark/benchmark.py \
    --backend vllm \
    --scenario "qwen3-8b-gpu" \
    --prompt all \
    --warmup 5 \
    --runs 20
```

#### –®–∞–≥ 2: –¢–µ—Å—Ç llama.cpp

```bash
# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å vLLM (Ctrl+C –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ 1)

# –¢–µ—Ä–º–∏–Ω–∞–ª 1: –ó–∞–ø—É—Å–∫ llama.cpp —Å–µ—Ä–≤–µ—Ä–∞
./llama.cpp/build/bin/llama-server \
    --model models/gguf/Qwen3-8B-Q4_K_M.gguf \
    --n-gpu-layers -1 \
    --port 8001

# –¢–µ—Ä–º–∏–Ω–∞–ª 2: –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞
source venv-vllm/bin/activate
# (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) cd –≤ –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞

python benchmark/benchmark.py \
    --backend llamacpp \
    --scenario "qwen3-8b-gpu" \
    --prompt all \
    --warmup 5 \
    --runs 20
```

#### –®–∞–≥ 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```bash
python benchmark/compare_results.py \
    benchmark/results/benchmark_qwen3-8b-gpu_*.json \
    benchmark/results/benchmark_qwen3-8b-gpu_*.json \
    --backend1 vllm \
    --backend2 llamacpp
```

### 2. –û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏)

–ï—Å–ª–∏ —É –≤–∞—Å >24GB VRAM –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏:

```bash
# –¢–µ—Ä–º–∏–Ω–∞–ª 1: vLLM –Ω–∞ –ø–æ—Ä—Ç—É 8000
vllm serve models/hf/Qwen3-8B-AWQ --port 8000 --gpu-memory-utilization 0.4

# –¢–µ—Ä–º–∏–Ω–∞–ª 2: llama.cpp –Ω–∞ –ø–æ—Ä—Ç—É 8001
./llama.cpp/build/bin/llama-server --model models/gguf/Qwen3-8B-Q4_K_M.gguf --n-gpu-layers -1 --port 8001

# –¢–µ—Ä–º–∏–Ω–∞–ª 3: –ë–µ–Ω—á–º–∞—Ä–∫ –æ–±–æ–∏—Ö
python benchmark/benchmark.py \
    --backend both \
    --scenario "qwen3-8b-comparison" \
    --prompt all
```

## üìù –ü–∞—Ä–∞–º–µ—Ç—Ä—ã benchmark.py

```bash
python benchmark/benchmark.py [OPTIONS]

Options:
  --backend <vllm|llamacpp|both>  –ö–∞–∫–æ–π –±—ç–∫–µ–Ω–¥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
  --vllm-url <url>                vLLM server URL (default: http://localhost:8000)
  --llamacpp-url <url>            llama.cpp server URL (default: http://localhost:8001)
  --prompt <short|medium|long|all> –¢–∏–ø –ø—Ä–æ–º–ø—Ç–∞ (default: all)
  --warmup <n>                    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥—Ä–µ–≤–æ—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (default: 5)
  --runs <n>                      –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–º–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (default: 20)
  --output <path>                 –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
  --scenario <name>               –ò–º—è —Å—Ü–µ–Ω–∞—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (default: custom)
```

## üìã –¢–∏–ø—ã –ø—Ä–æ–º–ø—Ç–æ–≤

### Short
- **–ü—Ä–æ–º–ø—Ç**: ~10 —Ç–æ–∫–µ–Ω–æ–≤
- **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è**: 100 —Ç–æ–∫–µ–Ω–æ–≤
- **–û–ø–∏—Å–∞–Ω–∏–µ**: "Explain quantum computing in simple terms."
- **–¶–µ–ª—å**: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π overhead, –±—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è

### Medium
- **–ü—Ä–æ–º–ø—Ç**: ~100 —Ç–æ–∫–µ–Ω–æ–≤
- **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è**: 300 —Ç–æ–∫–µ–Ω–æ–≤
- **–û–ø–∏—Å–∞–Ω–∏–µ**: –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ —Ä–∞–∑–≤–∏—Ç–∏–∏ AI
- **–¶–µ–ª—å**: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π use case –¥–ª—è —á–∞—Ç-–±–æ—Ç–æ–≤

### Long
- **–ü—Ä–æ–º–ø—Ç**: ~200 —Ç–æ–∫–µ–Ω–æ–≤
- **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è**: 500 —Ç–æ–∫–µ–Ω–æ–≤
- **–û–ø–∏—Å–∞–Ω–∏–µ**: –î–µ—Ç–∞–ª—å–Ω–æ–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É e-commerce
- **–¶–µ–ª—å**: –°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º

## üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ `results/benchmark_<scenario>_<timestamp>.json`:

```json
{
  "timestamp": "2026-01-09T03:00:00",
  "scenario": "qwen3-8b-gpu",
  "config": {
    "warmup_runs": 5,
    "measurement_runs": 20
  },
  "results": {
    "vllm": {
      "model": "Qwen/Qwen3-8B-AWQ",
      "prompts": {
        "short": {
          "ttft_ms": {"mean": 45.2, "std": 5.1, "p50": 44.0, "p95": 55.0},
          "tps": {"mean": 85.3, "std": 3.2, "p50": 85.0, "p95": 90.0},
          "e2e_latency_ms": {"mean": 1250.5, "std": 50.0},
          "successful_runs": 20,
          "total_runs": 20
        }
      }
    }
  }
}
```

## üîç –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### TTFT (Time to First Token)

**–•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: < 100ms
- vLLM –æ–±—ã—á–Ω–æ: 30-80ms
- llama.cpp –æ–±—ã—á–Ω–æ: 50-150ms

**–ß—Ç–æ –≤–ª–∏—è–µ—Ç**:
- –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞ (–ª–∏–Ω–µ–π–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å)
- Prefill optimization (vLLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Flash Attention)
- Batch size (–±–æ–ª—å—à–µ = –º–µ–¥–ª–µ–Ω–Ω–µ–µ TTFT)

### TPS (Tokens Per Second)

**–•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**:
- Small models (7-8B): 50-150 tokens/sec
- Large models (32B+): 10-40 tokens/sec

**–ß—Ç–æ –≤–ª–∏—è–µ—Ç**:
- –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (Q4 –±—ã—Å—Ç—Ä–µ–µ Q8, –Ω–æ —Ö—É–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ)
- GPU offloading (–±–æ–ª—å—à–µ —Å–ª–æ—ë–≤ –≤ GPU = –±—ã—Å—Ç—Ä–µ–µ)
- Memory bandwidth (–æ—Å–Ω–æ–≤–Ω–æ–π bottleneck)

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±—ç–∫–µ–Ω–¥–æ–≤

**vLLM –æ–±—ã—á–Ω–æ –±—ã—Å—Ç—Ä–µ–µ –∫–æ–≥–¥–∞**:
- Batch inference (–Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
- –î–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –ú–æ–¥–µ–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤ GPU

**llama.cpp –æ–±—ã—á–Ω–æ –±—ã—Å—Ç—Ä–µ–µ –∫–æ–≥–¥–∞**:
- Single request latency –∫—Ä–∏—Ç–∏—á–Ω–∞
- CPU/GPU –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º
- –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (Q4, Q2)

## üìä –ü—Ä–∏–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### Qwen3-8B (–ø–æ–ª–Ω–æ—Å—Ç—å—é –≤ GPU)

| –ú–µ—Ç—Ä–∏–∫–∞ | vLLM (AWQ 4-bit) | llama.cpp (Q4_K_M) | Winner |
|---------|------------------|---------------------|---------|
| TTFT (short) | 42ms | 75ms | vLLM |
| TPS (short) | 95 tok/s | 88 tok/s | vLLM |
| E2E (short) | 1100ms | 1200ms | vLLM |
| TTFT (long) | 180ms | 320ms | vLLM |
| TPS (long) | 82 tok/s | 85 tok/s | llama.cpp |

### Qwen3-32B (CPU offload)

| –ú–µ—Ç—Ä–∏–∫–∞ | vLLM (FP8) | llama.cpp (Q8_0) | Winner |
|---------|------------|-------------------|---------|
| TTFT | 250ms | 180ms | llama.cpp |
| TPS | 18 tok/s | 25 tok/s | llama.cpp |
| E2E | 28s | 20s | llama.cpp |

**–í—ã–≤–æ–¥**: llama.cpp —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –ø—Ä–∏ CPU offloading –±–ª–∞–≥–æ–¥–∞—Ä—è –ø–æ—Å–ª–æ–π–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é.

## üõ†Ô∏è Troubleshooting

### –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã failed

**–ü—Ä–æ–±–ª–µ–º–∞**: `Error: All runs failed`

**–ü—Ä–∏—á–∏–Ω—ã**:
1. –°–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—â–µ–Ω
2. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—Ç
3. –°–µ—Ä–≤–µ—Ä —É–ø–∞–ª –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ

**–†–µ—à–µ–Ω–∏–µ**:
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Å–µ—Ä–≤–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç
curl http://localhost:8000/v1/models  # vLLM
curl http://localhost:8001/v1/models  # llama.cpp

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ –æ—à–∏–±–∫–∏
```

### –í—ã—Å–æ–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

**–ü—Ä–æ–±–ª–µ–º–∞**: –ë–æ–ª—å—à–æ–π `std` (standard deviation)

**–ü—Ä–∏—á–∏–Ω—ã**:
1. –§–æ–Ω–æ–≤—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –Ω–∞ GPU
2. Thermal throttling
3. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ warmup runs

**–†–µ—à–µ–Ω–∏–µ**:
```bash
# –£–≤–µ–ª–∏—á–∏—Ç—å warmup
python benchmark/benchmark.py --warmup 10 --runs 30

# –ó–∞–∫—Ä—ã—Ç—å –¥—Ä—É–≥–∏–µ GPU –ø—Ä–æ—Ü–µ—Å—Å—ã
nvidia-smi  # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –µ—â—ë –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É
nvidia-smi -l 1  # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
```

### OOM –≤–æ –≤—Ä–µ–º—è –±–µ–Ω—á–º–∞—Ä–∫–∞

**–ü—Ä–æ–±–ª–µ–º–∞**: Out of memory error

**–†–µ—à–µ–Ω–∏–µ**:
```bash
# vLLM: —É–º–µ–Ω—å—à–∏—Ç—å gpu-memory-utilization
vllm serve model --gpu-memory-utilization 0.7

# llama.cpp: —É–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU —Å–ª–æ—ë–≤
./llama.cpp/build/bin/llama-server --model model.gguf --n-gpu-layers 20

# –ò–ª–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏
```

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

### –°—Ç–∞—Ç—å–∏ –æ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö

- [NVIDIA LLM Benchmarking Fundamentals](https://developer.nvidia.com/blog/llm-benchmarking-fundamental-concepts/)
- [Anyscale Benchmarking Metrics](https://docs.anyscale.com/llm/serving/benchmarking/metrics)
- [vLLM Performance Blog](https://blog.vllm.ai/2023/06/20/vllm.html)

### –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã

- [NVIDIA GenAI-Perf](https://github.com/NVIDIA/GenAI-Perf) - –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞
- [llm-perf-leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard) - –ü—É–±–ª–∏—á–Ω—ã–π leaderboard

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

- [vLLM Performance Tuning](https://docs.vllm.ai/en/stable/configuration/optimization/)
- [llama.cpp Performance Guide](https://github.com/ggml-org/llama.cpp/discussions/2094)

---

**–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è**: Streaming API (–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ TTFT)
**–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞**: mean, std, p50, p95, p99
**–§–æ—Ä–º–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**: JSON –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
